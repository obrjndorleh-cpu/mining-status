# ğŸ¯ QUALITY CONTROL WORKFLOW

Ensure only **clean, validated data** reaches your cloud database.

---

## ğŸ“‹ **NEW WORKFLOW (Production-Ready)**

```
Video Download â†’ Score â†’ Process â†’ Extract â†’ DELETE Video
                                       â†“
                                  LOCAL HDF5
                                       â†“
                                  INSPECT â† You review here!
                                       â†“
                              Approve / Reject
                                       â†“
                                  APPROVED/
                                       â†“
                              UPLOAD TO CLOUD â˜ï¸
```

**Key difference:** Human inspection before cloud upload!

---

## ğŸ” **STEP 1: INSPECT DATA**

### **Option A: Manual Inspection (Careful)**

Review each file individually:

```bash
# Inspect single file
python data_inspector.py --file data_mine/permanent_data/hdf5/some_file.hdf5

# You'll see:
# - Validation results (âœ… or âŒ)
# - Data quality stats
# - Warnings (if any)
# - Recommendation
```

### **Option B: Batch Inspection (Faster)**

Review all uninspected files:

```bash
# Interactive batch inspection
python data_inspector.py --batch

# For each file, you'll see:
# - Validation results
# - Quality stats
# - Recommendation
#
# Then decide: [a]pprove / [r]eject / [s]kip
```

### **Option C: Auto-Approve (Production Mode)**

Auto-approve files that pass all checks:

```bash
# Auto-approve clean files, manual review for questionable ones
python data_inspector.py --batch --auto-approve --auto-reject

# This will:
# - âœ… Auto-approve files with no errors/warnings
# - âŒ Auto-reject files with critical errors
# - â¸ï¸  Ask you to manually review borderline cases
```

---

## âœ… **WHAT GETS CHECKED:**

### **Automatic Validation:**

1. **File Integrity**
   - Can file be opened?
   - All required datasets present?
   - No corruption?

2. **Pose Data Quality**
   - All 33 keypoints present?
   - No NaN or Inf values?
   - Average visibility > 30%?

3. **Hand Data** (if present)
   - Complete keypoint data?
   - Valid coordinate ranges?

4. **Metadata**
   - Action classification available?
   - Confidence scores present?

### **Recommendation Logic:**

- **APPROVE**: No errors, no warnings â†’ Auto-uploadable
- **APPROVE_WITH_WARNINGS**: Valid but has minor issues â†’ Review recommended
- **REJECT**: Critical errors or low quality â†’ Don't upload

---

## ğŸ“ **FILE ORGANIZATION**

After inspection, files are organized:

```
data_mine/permanent_data/
â”œâ”€â”€ hdf5/              â† Uninspected (raw extracted data)
â”œâ”€â”€ approved/          â† Ready for cloud upload âœ…
â”œâ”€â”€ rejected/          â† Quality issues, not uploaded âŒ
â”œâ”€â”€ json/              â† Metadata files
â””â”€â”€ inspection_log.json â† History of all inspections
```

---

## â˜ï¸ **STEP 2: UPLOAD APPROVED DATA**

### **Safe Upload (Only Approved Files)**

```bash
# Upload only inspected and approved data
python upload_approved_data.py

# This uploads from: data_mine/permanent_data/approved/
# Skips: Uninspected and rejected files
```

### **What Happens:**

1. Reads files from `approved/` directory
2. Validates they were inspected
3. Uploads to MongoDB Cloud
4. Shows upload summary

---

## ğŸ”„ **DAILY WORKFLOW**

### **Option 1: Review Daily**

```bash
# Morning routine:
python data_inspector.py --batch --auto-approve --auto-reject
python upload_approved_data.py
```

### **Option 2: Review Weekly**

Let data accumulate, review in batch:

```bash
# Weekly cleanup:
python data_inspector.py --batch --auto-approve
python upload_approved_data.py

# Manually review borderline cases
```

---

## ğŸ“Š **CHECK INSPECTION STATUS**

```bash
# See what's been inspected
cat data_mine/permanent_data/inspection_log.json

# Count files by status
ls data_mine/permanent_data/hdf5/*.hdf5 | wc -l      # Uninspected
ls data_mine/permanent_data/approved/*.hdf5 | wc -l  # Approved
ls data_mine/permanent_data/rejected/*.hdf5 | wc -l  # Rejected
```

---

## ğŸ¯ **EXAMPLE SESSION**

```bash
$ python data_inspector.py --batch

======================================================================
BATCH INSPECTION: 35 files
======================================================================

[1/35]
======================================================================
INSPECTING: video_001.hdf5
======================================================================

Validation: âœ… PASSED

Stats:
  â€¢ pose_keypoints: 33
  â€¢ avg_visibility: 0.85
  â€¢ visible_keypoints: 31
  â€¢ left_hand: True
  â€¢ right_hand: True

Metadata:
  â€¢ Action: reaching
  â€¢ Confidence: 0.92
  â€¢ Method: vision

âœ… RECOMMENDATION: APPROVE for cloud upload

----------------------------------------------------------------------
Decision [a]pprove / [r]eject / [s]kip: a
âœ… Approved: video_001.hdf5

[2/35]
======================================================================
INSPECTING: video_002.hdf5
======================================================================

Validation: âœ… PASSED

Warnings:
  âš ï¸  Low average visibility: 0.28

Stats:
  â€¢ pose_keypoints: 33
  â€¢ avg_visibility: 0.28
  â€¢ visible_keypoints: 12
  â€¢ left_hand: False
  â€¢ right_hand: False

âš ï¸  RECOMMENDATION: APPROVE (with warnings)

----------------------------------------------------------------------
Decision [a]pprove / [r]eject / [s]kip: r
âŒ Rejected: video_002.hdf5

...

======================================================================
INSPECTION COMPLETE
======================================================================

Inspection Summary:
  Total inspected: 35
  Approved: 28
  Rejected: 7

Approved files ready for cloud upload at:
  data_mine/permanent_data/approved
```

---

## ğŸ’¡ **BEST PRACTICES**

### **1. Inspect Regularly**

Don't let uninspected files accumulate:

```bash
# Add to crontab (daily at 8am):
0 8 * * * cd /path/to/project && python data_inspector.py --batch --auto-approve
```

### **2. Review Rejected Files**

Check why files were rejected to improve mining:

```bash
ls data_mine/permanent_data/rejected/

# If too many rejections, adjust:
# - Quality threshold (currently 70)
# - Acceptance criteria
```

### **3. Backup Before Upload**

Keep local copies of approved data:

```bash
# Backup approved data before cloud upload
cp -r data_mine/permanent_data/approved/ backups/$(date +%Y%m%d)_approved/
python upload_approved_data.py
```

### **4. Monitor Cloud Storage**

Check what's in cloud regularly:

```bash
python cloud_mining_setup.py --status
```

---

## ğŸ”’ **QUALITY GUARANTEES**

With this workflow, your cloud database contains:

âœ… **Only validated files** (passed integrity checks)
âœ… **Human-reviewed data** (or auto-approved if perfect)
âœ… **No corrupted data** (automatic detection)
âœ… **Consistent quality** (visibility thresholds)
âœ… **Traceable history** (inspection log)

---

## ğŸš€ **INTEGRATION WITH EXISTING PIPELINE**

Your video mining continues as before:

```
Video Mining (24/7) â†’ Extracts HDF5 â†’ Saves to hdf5/
                                           â†“
                                     (accumulates)
                                           â†“
                      You inspect when ready â†’ approved/
                                           â†“
                           Upload to cloud when clean
```

**Nothing changes in mining operation!** Just adds quality control before cloud.

---

## ğŸ“ˆ **EXPECTED WORKFLOW**

**Daily:**
- Mining runs 24/7 (no change)
- ~390 videos processed/day
- ~35 HDF5 files generated/day

**Weekly:**
- Inspect accumulated files: `python data_inspector.py --batch --auto-approve`
- Upload approved: `python upload_approved_data.py`
- Review rejected files
- ~245 files/week to cloud (assuming 30% rejection in inspection)

---

**Your data is now production-ready with quality control!** ğŸ‰
